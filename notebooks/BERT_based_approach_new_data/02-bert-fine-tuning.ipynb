{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:45:44.991195Z",
     "iopub.status.busy": "2025-05-15T09:45:44.990917Z",
     "iopub.status.idle": "2025-05-15T09:45:44.996041Z",
     "shell.execute_reply": "2025-05-15T09:45:44.995346Z",
     "shell.execute_reply.started": "2025-05-15T09:45:44.991175Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset,load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,TrainingArguments, Trainer,logging\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:45:44.997597Z",
     "iopub.status.busy": "2025-05-15T09:45:44.997307Z",
     "iopub.status.idle": "2025-05-15T09:45:45.008327Z",
     "shell.execute_reply": "2025-05-15T09:45:45.007695Z",
     "shell.execute_reply.started": "2025-05-15T09:45:44.997577Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "logging.enable_progress_bar()  \n",
    "logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:45:45.009874Z",
     "iopub.status.busy": "2025-05-15T09:45:45.009660Z",
     "iopub.status.idle": "2025-05-15T09:45:45.568117Z",
     "shell.execute_reply": "2025-05-15T09:45:45.567525Z",
     "shell.execute_reply.started": "2025-05-15T09:45:45.009859Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>End of eviction moratorium means millions of A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>End of eviction moratorium means millions of A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>End of eviction moratorium means millions of A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>End of eviction moratorium means millions of A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>End of eviction moratorium means millions of A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       1  End of eviction moratorium means millions of A...\n",
       "1       1  End of eviction moratorium means millions of A...\n",
       "2       1  End of eviction moratorium means millions of A...\n",
       "3       1  End of eviction moratorium means millions of A...\n",
       "4       1  End of eviction moratorium means millions of A..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/new_data/processed/ready_data.csv')\n",
    "df['target'] = df['target'].astype(int)\n",
    "df['text']=df['statement']+\" \"+df['tweet']\n",
    "df = df.drop(['statement','tweet'],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:45:45.569004Z",
     "iopub.status.busy": "2025-05-15T09:45:45.568784Z",
     "iopub.status.idle": "2025-05-15T09:45:45.632025Z",
     "shell.execute_reply": "2025-05-15T09:45:45.631526Z",
     "shell.execute_reply.started": "2025-05-15T09:45:45.568987Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df, tmp_df = train_test_split(\n",
    "    df, test_size=0.20, stratify=df['target'], random_state=4)\n",
    "val_df, test_df  = train_test_split(\n",
    "    tmp_df, test_size=0.50, stratify=tmp_df['target'], random_state=42)\n",
    "original_test_labels = test_df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:45:45.633688Z",
     "iopub.status.busy": "2025-05-15T09:45:45.633446Z",
     "iopub.status.idle": "2025-05-15T09:45:45.810690Z",
     "shell.execute_reply": "2025-05-15T09:45:45.809815Z",
     "shell.execute_reply.started": "2025-05-15T09:45:45.633671Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:45:45.811813Z",
     "iopub.status.busy": "2025-05-15T09:45:45.811582Z",
     "iopub.status.idle": "2025-05-15T09:45:46.182409Z",
     "shell.execute_reply": "2025-05-15T09:45:46.181871Z",
     "shell.execute_reply.started": "2025-05-15T09:45:45.811796Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\ignat\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\ignat\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\ignat\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\ignat\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\ignat\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\ignat\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Fake\",\n",
      "    \"1\": \"Real\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Fake\": 0,\n",
      "    \"Real\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\ignat\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\model.safetensors\n",
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "tokenizer= AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "id2label = {0:\"Fake\", 1:\"Real\"}\n",
    "label2id = {\"Fake\": 0, \"Real\":1}\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, \n",
    "                                                           num_labels=2,\n",
    "                                                          id2label = id2label,\n",
    "                                                          label2id = label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:45:46.189905Z",
     "iopub.status.busy": "2025-05-15T09:45:46.189637Z",
     "iopub.status.idle": "2025-05-15T09:46:05.818291Z",
     "shell.execute_reply": "2025-05-15T09:46:05.817548Z",
     "shell.execute_reply.started": "2025-05-15T09:45:46.189862Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/107354 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 107354/107354 [00:14<00:00, 7341.29 examples/s]\n",
      "Map: 100%|██████████| 13419/13419 [00:01<00:00, 8532.10 examples/s]\n",
      "Map: 100%|██████████| 13420/13420 [00:02<00:00, 6696.74 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(data):\n",
    "    return tokenizer(data[\"text\"])\n",
    "train_tokenized = train_dataset.map(preprocess_function, batched=True)\n",
    "val_tokenized = val_dataset.map(preprocess_function, batched=True)\n",
    "test_tokenized = test_dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:46:06.977385Z",
     "iopub.status.busy": "2025-05-15T09:46:06.976893Z",
     "iopub.status.idle": "2025-05-15T09:46:06.990391Z",
     "shell.execute_reply": "2025-05-15T09:46:06.989803Z",
     "shell.execute_reply.started": "2025-05-15T09:46:06.977333Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_tokenized = train_tokenized.rename_column(\"target\",\"labels\")\n",
    "val_tokenized = val_tokenized.rename_column(\"target\",\"labels\")\n",
    "test_tokenized = test_tokenized.rename_column(\"target\",\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:46:05.819456Z",
     "iopub.status.busy": "2025-05-15T09:46:05.819168Z",
     "iopub.status.idle": "2025-05-15T09:46:05.822973Z",
     "shell.execute_reply": "2025-05-15T09:46:05.822249Z",
     "shell.execute_reply.started": "2025-05-15T09:46:05.819433Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:45:46.183399Z",
     "iopub.status.busy": "2025-05-15T09:45:46.183154Z",
     "iopub.status.idle": "2025-05-15T09:45:46.188696Z",
     "shell.execute_reply": "2025-05-15T09:45:46.187903Z",
     "shell.execute_reply.started": "2025-05-15T09:45:46.183374Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for name, param in model.base_model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for name, param in model.base_model.named_parameters():\n",
    "    if \"pooler\" in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:46:05.825095Z",
     "iopub.status.busy": "2025-05-15T09:46:05.824906Z",
     "iopub.status.idle": "2025-05-15T09:46:06.928684Z",
     "shell.execute_reply": "2025-05-15T09:46:06.927917Z",
     "shell.execute_reply.started": "2025-05-15T09:46:05.825081Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "auc_score = evaluate.load(\"roc_auc\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    probabilities = np.exp(predictions) / np.exp(predictions).sum(-1, keepdims=True)\n",
    "    positive_class_probs = probabilities[:, 1]\n",
    "    auc = np.round(auc_score.compute(prediction_scores=positive_class_probs, references=labels)['roc_auc'],3)\n",
    "    \n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    acc = np.round(accuracy.compute(predictions=predicted_classes, references=labels)['accuracy'],3)\n",
    "    \n",
    "    return {\"Accuracy\": acc, \"AUC\": auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:46:06.929588Z",
     "iopub.status.busy": "2025-05-15T09:46:06.929394Z",
     "iopub.status.idle": "2025-05-15T09:46:06.970440Z",
     "shell.execute_reply": "2025-05-15T09:46:06.969744Z",
     "shell.execute_reply.started": "2025-05-15T09:46:06.929574Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# lr = 2e-5\n",
    "# batch_size = 32\n",
    "# num_epochs = 5\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"/kaggle/working/real/bert-fakenews_classifier\",\n",
    "#     learning_rate=lr,\n",
    "#     per_device_train_batch_size=batch_size,\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "#     num_train_epochs=num_epochs,\n",
    "#     logging_strategy=\"steps\",\n",
    "#     eval_strategy=\"steps\",\n",
    "#     eval_steps=1000,\n",
    "#     logging_steps=50,\n",
    "#     save_strategy=\"steps\",\n",
    "#     save_steps=2000,\n",
    "#     load_best_model_at_end=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:46:06.971508Z",
     "iopub.status.busy": "2025-05-15T09:46:06.971234Z",
     "iopub.status.idle": "2025-05-15T09:46:06.976064Z",
     "shell.execute_reply": "2025-05-15T09:46:06.975410Z",
     "shell.execute_reply.started": "2025-05-15T09:46:06.971486Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['labels', 'text', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "print(train_tokenized[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:46:06.991576Z",
     "iopub.status.busy": "2025-05-15T09:46:06.991327Z",
     "iopub.status.idle": "2025-05-15T09:46:06.996134Z",
     "shell.execute_reply": "2025-05-15T09:46:06.995414Z",
     "shell.execute_reply.started": "2025-05-15T09:46:06.991555Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['labels', 'text', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "print(test_tokenized[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:46:06.997485Z",
     "iopub.status.busy": "2025-05-15T09:46:06.996983Z",
     "iopub.status.idle": "2025-05-15T09:46:07.006602Z",
     "shell.execute_reply": "2025-05-15T09:46:07.005917Z",
     "shell.execute_reply.started": "2025-05-15T09:46:06.997470Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "##trainer = Trainer(\n",
    "##    args=training_args,\n",
    "#    train_dataset=train_tokenized,\n",
    "#    eval_dataset=val_tokenized,\n",
    "#    tokenizer=tokenizer,\n",
    "#    data_collator=data_collator,\n",
    "#    compute_metrics=compute_metrics,\n",
    "#)\n",
    "\n",
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:46:07.007721Z",
     "iopub.status.busy": "2025-05-15T09:46:07.007460Z",
     "iopub.status.idle": "2025-05-15T09:46:07.017654Z",
     "shell.execute_reply": "2025-05-15T09:46:07.016972Z",
     "shell.execute_reply.started": "2025-05-15T09:46:07.007700Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#import shutil\n",
    "\n",
    "#shutil.make_archive(\n",
    "  #  \"/kaggle/working/bert_classifier_backup_8000\",  \n",
    "  # 'zip',\n",
    "  #  \"/kaggle/working/real/bert-fakenews_classifier/checkpoint-8000\"  \n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained model upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:46:07.018690Z",
     "iopub.status.busy": "2025-05-15T09:46:07.018348Z",
     "iopub.status.idle": "2025-05-15T09:46:07.131991Z",
     "shell.execute_reply": "2025-05-15T09:46:07.131498Z",
     "shell.execute_reply.started": "2025-05-15T09:46:07.018667Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\ignat\\.cache\\huggingface\\hub\\models--Igoras6534--fine_tuned_BERT_fakenews_8000\\snapshots\\979892c3a826a4776928476fa4a23a6876cbf601\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Fake\",\n",
      "    \"1\": \"Real\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Fake\": 0,\n",
      "    \"Real\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\ignat\\.cache\\huggingface\\hub\\models--Igoras6534--fine_tuned_BERT_fakenews_8000\\snapshots\\979892c3a826a4776928476fa4a23a6876cbf601\\model.safetensors\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at Igoras6534/fine_tuned_BERT_fakenews_8000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"Igoras6534/fine_tuned_BERT_fakenews_8000\"  \n",
    "\n",
    "model_ready = AutoModelForSequenceClassification.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:46:07.132795Z",
     "iopub.status.busy": "2025-05-15T09:46:07.132625Z",
     "iopub.status.idle": "2025-05-15T09:46:07.334469Z",
     "shell.execute_reply": "2025-05-15T09:46:07.333600Z",
     "shell.execute_reply.started": "2025-05-15T09:46:07.132783Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ignat\\AppData\\Local\\Temp\\ipykernel_21588\\4049891494.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model_ready,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:46:18.780924Z",
     "iopub.status.busy": "2025-05-15T09:46:18.780653Z",
     "iopub.status.idle": "2025-05-15T09:47:47.829977Z",
     "shell.execute_reply": "2025-05-15T09:47:47.829350Z",
     "shell.execute_reply.started": "2025-05-15T09:46:18.780905Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 13420\n",
      "  Batch size = 8\n",
      "c:\\Users\\ignat\\source\\repos\\Fake-News_Detector\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': np.float64(0.873), 'AUC': np.float64(0.943)}\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(test_tokenized.remove_columns([\"labels\"]))\n",
    "logits = predictions.predictions\n",
    "metrics = compute_metrics((logits, original_test_labels.reset_index(drop=True)))\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T10:12:36.401323Z",
     "iopub.status.busy": "2025-05-15T10:12:36.400738Z",
     "iopub.status.idle": "2025-05-15T10:12:36.536972Z",
     "shell.execute_reply": "2025-05-15T10:12:36.536294Z",
     "shell.execute_reply.started": "2025-05-15T10:12:36.401300Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tekst: Scientists have discovered that the latest 5G networks are secretly altering human DNA, causing irreversible mutations that could affect generations to come. Despite official statements claiming safety, whistleblowers inside telecom companies reveal that the government is hiding the truth to push the rollout faster. Early symptoms include chronic fatigue, memory loss, and mysterious skin rashes appearing in cities with new 5G towers. Experts warn this could trigger a public health crisis worse than any pandemic we've seen before. Stay informed and protect yourself from this invisible threat.\n",
      "Przewidywana klasa: Fake\n",
      "Prawdopodobieństwa: [0.6091264486312866, 0.3908735513687134]\n"
     ]
    }
   ],
   "source": [
    "model.eval() \n",
    "\n",
    "sample_text=\"Scientists have discovered that the latest 5G networks are secretly altering human DNA, causing irreversible mutations that could affect generations to come. Despite official statements claiming safety, whistleblowers inside telecom companies reveal that the government is hiding the truth to push the rollout faster. Early symptoms include chronic fatigue, memory loss, and mysterious skin rashes appearing in cities with new 5G towers. Experts warn this could trigger a public health crisis worse than any pandemic we've seen before. Stay informed and protect yourself from this invisible threat.\"\n",
    "inputs = tokenizer(sample_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "predicted_class_idx = torch.argmax(probs, dim=-1).item()\n",
    "\n",
    "print(f\"Tekst: {sample_text}\")\n",
    "print(f\"Przewidywana klasa: {id2label.get(predicted_class_idx, str(predicted_class_idx))}\")\n",
    "print(f\"Prawdopodobieństwa: {probs.squeeze().tolist()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7418626,
     "sourceId": 11811799,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 344760,
     "modelInstanceId": 323968,
     "sourceId": 393805,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
